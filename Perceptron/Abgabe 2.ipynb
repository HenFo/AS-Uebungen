{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Systeme - Hausaufgabe 2\n",
    "Henry Fock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbau und Formeln für ein neuronales Netz\n",
    "Ein neuronales Netz ist aus mehreren Komponenten aufgebaut:\n",
    "- Vollvernetzte (lineare) Schicht. Dies entspricht dem Matrix - Vektorprodukt von Gewichtematrix und Eingabevektor. Die Zeilen der Gewichtematrix entsprechen den Gewichten der Eingangsneuronen (Spaltenvektor): $$f(x) = W^Tx$$\n",
    "- (Nicht lineare) Aktivierungs-/ Transferfunktion. Hier wird die Fermi-/ Sigmoidfunktion verwendet: $$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "- Fehlerfunktion. Hier wird zum einen die Categorical Cross Entropy und der Mean Squered Error verwendet. Dabei ist $x$ das Ergebnis des Netzes und $y$ der Zielwert. $x$ und $y$ sind jeweils Spaltenvektoren.\n",
    "\\begin{align*}\n",
    "\\text{MSE}(x,y) &= \\frac{1}{|x|}\\sum_{i=0}^{|x|} \\left(x_i - y_i \\right)^2 \\\\\n",
    "\\text{CCE}(x,y) &= -\\sum_{i=0}^{|x|} y_i \\cdot \\log(x_i)\n",
    "\\end{align*}\n",
    "\n",
    "Ein Forward-pass, bis zum Fehler $E$ über eine Schicht entspricht:\n",
    "$$E\\left(\\sigma\\left(f\\left(x\\right)\\right)\\right)$$\n",
    "Über zwei Schichten:\n",
    "$$E\\left(\\sigma_2\\left(f_2\\left(\\sigma_1\\left(f_1\\left(x\\right)\\right)\\right)\\right)\\right)$$\n",
    "\n",
    "### Formeln für Backpropegation\n",
    "Außerdem werden die partiellen Ableitungen für das Gradientenabstiegsverfahren der einzelnen Funktionen benötigt. Die lineare Schicht benötigt eine mit Bezug auf die Gewichte, um diese zu ändern und eine mit Bezug auf den Eingang, um den Gradienten in die darüberliegenden Schichten weiterzuleiten.\n",
    "Die Gradienten lassen sich über die Kettenregel berechnen:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_{ij}^2} &= \\frac{\\partial f_2}{\\partial w_{ij}^2} \\cdot \\frac{\\partial \\sigma_2}{\\partial f_2} \\cdot \\frac{\\partial E}{\\partial \\sigma_2}  \\\\\n",
    "\\delta_2 &= \\frac{\\partial \\sigma_2}{\\partial f_2} \\cdot \\frac{\\partial E}{\\partial \\sigma_2} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{ij}^1} &= \\frac{\\partial f_1}{\\partial w_{ij}^1}\\cdot \\frac{\\partial \\sigma_1}{\\partial f_1} \\cdot \\frac{\\partial f_2}{\\partial \\sigma_1} \\cdot \\delta_2 \n",
    "\\end{align*}\n",
    "\n",
    "Die Formeln für die Gradienten sind:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial w_{ij}} &= x_i \\\\\n",
    "\\frac{\\partial f_2}{\\partial \\sigma_1} &= W \\\\\n",
    "\\frac{\\partial \\sigma}{\\partial f} &= \\sigma(f)\\cdot (1 - \\sigma(f)) \\\\\n",
    "\\frac{\\partial \\text{MSE}}{\\partial \\sigma_i} &= \\frac{1}{|\\sigma|}\\cdot 2(\\sigma_i - y_i) \\\\\n",
    "\\frac{\\partial \\text{CCE}}{\\partial \\sigma_i} &= -\\frac{y}{\\sigma_i}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung: Definieren von abstrakten Oberklassen für Schichten Fehler und Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray, train: bool = False) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def backwards(self, prev_grads: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, lr: float = 0.01) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def loss(self, x: np.ndarray, y: np.ndarray, train: bool = False) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def backwards(self) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Metric(ABC):\n",
    "    @abstractmethod\n",
    "    def metric(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int = 10,\n",
    "        lr: float = 0.01,\n",
    "        batch_size: int = 1,\n",
    "        loss_threshold: float = 1e-2,\n",
    "        shuffle: bool = True,\n",
    "    ) -> None:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung: Implementieren der Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, num_inputs: int, num_outputs: int, weight_init:str = \"uniform\") -> None:\n",
    "        assert weight_init in (\"uniform\", \"normal\")\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_inputs = num_outputs\n",
    "\n",
    "        if weight_init == \"uniform\":\n",
    "            self.weights = np.random.rand(num_inputs + 1, num_outputs)\n",
    "        if weight_init == \"normal\":\n",
    "            self.weights = np.random.randn(num_inputs + 1, num_outputs)\n",
    "        \n",
    "        # self.weights *= np.sqrt(1.0 / num_inputs)\n",
    "\n",
    "        self.weight_gradients: np.ndarray = None\n",
    "\n",
    "        self.train_input: np.ndarray = None\n",
    "        self.train_output: np.ndarray = None\n",
    "\n",
    "    def forward(self, x: np.ndarray, train: bool = False) -> np.ndarray:\n",
    "        x_bias = np.vstack([np.ones((1, 1)), x])\n",
    "        out = np.dot(self.weights.T, x_bias)\n",
    "        if train:\n",
    "            self.train_input = x_bias\n",
    "            self.train_output = out\n",
    "        return out\n",
    "\n",
    "    def backwards(self, prev_grads: np.ndarray) -> np.ndarray:\n",
    "        self.weight_gradients = np.dot(prev_grads, self.train_input.T).T\n",
    "        return np.dot(self.weights[1:], prev_grads)\n",
    "\n",
    "    def train(self, lr: float = 0.01) -> None:\n",
    "        self.weights -= lr * self.weight_gradients\n",
    "        self.weight_gradients = None\n",
    "        self.train_input = None\n",
    "        self.train_output = None\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        self.train_input: np.ndarray = None\n",
    "        self.train_output: np.ndarray = None\n",
    "\n",
    "    def forward(self, x: np.ndarray, train: bool = False) -> np.ndarray:\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        if train:\n",
    "            self.train_input = x\n",
    "            self.train_output = out\n",
    "        return out\n",
    "\n",
    "    def backwards(self, prev_grads: np.ndarray) -> np.ndarray:\n",
    "        grads = self.train_output * (1 - self.train_output)\n",
    "        return prev_grads * grads\n",
    "\n",
    "    def train(self, *_) -> None:\n",
    "        self.train_input = None\n",
    "        self.train_output = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung: Implementieren der Fehlerfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquereError(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        self.train_x: np.ndarray = 0\n",
    "        self.train_y: np.ndarray = 0\n",
    "\n",
    "    def loss(self, x: np.ndarray, y: np.ndarray, train: bool = False) -> float:\n",
    "        if train:\n",
    "            self.train_x = x\n",
    "            self.train_y = y\n",
    "        return np.mean(np.power(x - y, 2))\n",
    "\n",
    "    def backwards(self) -> np.ndarray:\n",
    "        out = 2/self.train_y.size*(self.train_x - self.train_y)\n",
    "        return out\n",
    "\n",
    "class CategoricalCrossEntropy(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        self.train_x: np.ndarray = 0\n",
    "        self.train_y: np.ndarray = 0\n",
    "\n",
    "    def loss(self, x: np.ndarray, y: np.ndarray, train: bool = False) -> float:\n",
    "        if train:\n",
    "            self.train_x = x\n",
    "            self.train_y = y\n",
    "        return -np.sum(y * np.log(x))\n",
    "\n",
    "    def backwards(self) -> np.ndarray:\n",
    "        out = -(self.train_y / self.train_x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        self.train_x: np.ndarray = 0\n",
    "        self.train_y: np.ndarray = 0\n",
    "\n",
    "    def loss(self, x: np.ndarray, y: np.ndarray, train: bool = False) -> float:\n",
    "        if train:\n",
    "            self.train_x = x\n",
    "            self.train_y = y\n",
    "        return (1/y.size) * np.sum(-(y * np.log(x) + (1-y) * np.log(1-x)))\n",
    "\n",
    "    def backwards(self) -> np.ndarray:\n",
    "        out = (1/self.train_y.size) * ((self.train_x - self.train_y) / ((1 - self.train_x) * self.train_x))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung: Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "\tdef metric(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "\t\tx = x.astype(\"uint8\")\n",
    "\t\ty = y.astype(\"uint8\")\n",
    "\t\treturn np.mean(x == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung: Trainingsroutine im MultilayerPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helferklasse fürs Datenhandling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    @classmethod\n",
    "    def shuffle(self, x: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray]:\n",
    "        sampling = np.random.permutation(x.shape[0])\n",
    "        return x[sampling], y[sampling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layers: List[Layer], loss: Loss) -> None:\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        results = np.array([self._forward(sample, train=False) for sample in x])\n",
    "        return results\n",
    "\n",
    "    def _forward(self, x: np.ndarray, train: bool) -> np.ndarray:\n",
    "        intermed_result: np.ndarray = x\n",
    "        for layer in self.layers:\n",
    "            intermed_result = layer.forward(intermed_result, train=train)\n",
    "        return intermed_result\n",
    "\n",
    "    def _backpropegate(self) -> None:\n",
    "        gradient = self.loss_function.backwards()\n",
    "        for layer in self.layers[::-1]:\n",
    "            gradient = layer.backwards(gradient)\n",
    "\n",
    "    def _update_weights(self, lr: float = 0.01) -> None:\n",
    "        for layer in self.layers[::-1]:\n",
    "            layer.train(lr)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int = 10,\n",
    "        lr: float = 0.01,\n",
    "        min_delta: float = 1e-5,\n",
    "        patience: int = 0,\n",
    "        loss_threshold:float = 1e-2,\n",
    "        shuffle: bool = True\n",
    "    ) -> tuple[int, str]:\n",
    "        epoch: int = 1\n",
    "        last_loss = 1e5\n",
    "        patience_counter = 0\n",
    "        while epoch <= epochs:\n",
    "            xs, ys = x, y\n",
    "            if shuffle:\n",
    "                xs, ys = Dataset.shuffle(x, y)\n",
    "            for sample_x, sample_y in zip(xs, ys):\n",
    "                result = self._forward(sample_x, train=True)\n",
    "                self.loss_function.loss(result, sample_y, train=True)\n",
    "\n",
    "                self._backpropegate()\n",
    "                self._update_weights(lr)\n",
    "\n",
    "\n",
    "            intermed_preds = self.predict(x)\n",
    "            intermed_loss = self.loss_function.loss(intermed_preds, y)\n",
    "\n",
    "            if loss_threshold > intermed_loss:\n",
    "                return (epoch, \"Fehlergrenze\")\n",
    "\n",
    "            if min_delta > 0:\n",
    "                loss_delta = abs(last_loss - intermed_loss)\n",
    "                last_loss = intermed_loss\n",
    "                patience_counter += 1\n",
    "\n",
    "                if loss_delta < min_delta:  \n",
    "                    if patience_counter > patience:\n",
    "                        return (epoch, \"Konvergenz\")\n",
    "                else:\n",
    "                    patience_counter = 0\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        return (epoch - 1, \"Abbruch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellen von Helferklassen zur Datenrepräsentation\n",
    "Zuordnung der Segmente mit ihren Indizes:\n",
    "\n",
    "<img src=\"Images/d8.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SevenSegment:\n",
    "\tdef __init__(self, encoding: np.ndarray) -> None:\n",
    "\t\tassert encoding.size == 7\n",
    "\t\tself.encoding = encoding\n",
    "\n",
    "\tdef display(self) -> None:\n",
    "\t\timg = self.getImage()\n",
    "\t\tplt.imshow(img)\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef getImage(self) -> np.ndarray:\n",
    "\t\tsegments = self.getEncoding()\n",
    "\t\tassert segments.size == 7\n",
    "\n",
    "\t\timage = np.zeros((5, 3), dtype=np.uint8)\n",
    "\t\tsegment_to_image = {\n",
    "\t\t\t0: {\"x\":[0, 0, 0], \"y\":[0, 1, 2]},\n",
    "\t\t\t1: {\"x\":[0, 1, 2], \"y\":[0, 0, 0]},\n",
    "\t\t\t2: {\"x\":[0, 1, 2], \"y\":[2, 2, 2]},\n",
    "\t\t\t3: {\"x\":[2, 2, 2], \"y\":[0, 1, 2]},\n",
    "\t\t\t4: {\"x\":[2, 3, 4], \"y\":[0, 0, 0]},\n",
    "\t\t\t5: {\"x\":[2, 3, 4], \"y\":[2, 2, 2]},\n",
    "\t\t\t6: {\"x\":[4, 4, 4], \"y\":[0, 1, 2]},\n",
    "\t\t}\n",
    "\n",
    "\t\tfor i, segment in enumerate(segments):\n",
    "\t\t\tif segment == 1:\n",
    "\t\t\t\ts2i = segment_to_image[i]\n",
    "\t\t\t\timage[s2i[\"x\"], s2i[\"y\"]] = 1\n",
    "\n",
    "\t\treturn image\n",
    "\n",
    "\tdef getEncoding(self) -> np.ndarray:\n",
    "\t\treturn self.encoding\n",
    "\n",
    "\n",
    "def seven_segment_factory_method(char: str) -> SevenSegment:\n",
    "\tassert char in (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"A\", \"b\", \"C\", \"d\", \"E\", \"F\")\n",
    "\tif char == \"0\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 1, 0, 1, 1, 1]))\n",
    "\tif char == \"1\":\n",
    "\t\treturn SevenSegment(np.array([0, 1, 0, 0, 1, 0, 0]))\n",
    "\tif char == \"2\":\n",
    "\t\treturn SevenSegment(np.array([1, 0, 1, 1, 1, 0, 1]))\n",
    "\tif char == \"3\":\n",
    "\t\treturn SevenSegment(np.array([1, 0, 1, 1, 0, 1, 1]))\n",
    "\tif char == \"4\":\n",
    "\t\treturn SevenSegment(np.array([0, 1, 1, 1, 0, 1, 0]))\n",
    "\tif char == \"5\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 0, 1, 0, 1, 1]))\n",
    "\tif char == \"6\":\n",
    "\t\treturn SevenSegment(np.array([0, 1, 0, 1, 1, 1, 1]))\n",
    "\tif char == \"7\":\n",
    "\t\treturn SevenSegment(np.array([1, 0, 1, 0, 0, 1, 0]))\n",
    "\tif char == \"8\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 1, 1, 1, 1, 1]))\n",
    "\tif char == \"9\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 1, 1, 0, 1, 0]))\n",
    "\tif char == \"A\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 1, 1, 1, 1, 0]))\n",
    "\tif char == \"b\":\n",
    "\t\treturn SevenSegment(np.array([0, 1, 0, 1, 1, 1, 1]))\n",
    "\tif char == \"C\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 0, 0, 1, 0, 1]))\n",
    "\tif char == \"d\":\n",
    "\t\treturn SevenSegment(np.array([0, 0, 1, 1, 1, 1, 1]))\n",
    "\tif char == \"E\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 0, 1, 1, 0, 1]))\n",
    "\tif char == \"F\":\n",
    "\t\treturn SevenSegment(np.array([1, 1, 0, 1, 1, 0, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seven_segment_factory_method(\"A\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1\n",
    "Untersuchen Sie wie die Lernf¨ahigkeit und -geschwindigkeit des Netz von\n",
    "\n",
    "- der Anzahl der zu erlernenden Muster d.h. 0 – 9 oder 0 – 15...\n",
    "- der Anzahl der Eingabeneuronen, also entweder 4 oder 10/16...\n",
    "- der Anzahl der verborgenen Neuronen Schichten...\n",
    "- der Anzahl der Neuronen in der jeweiligen Schicht...\n",
    "\n",
    "...abhängt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erzeugen des Trainingsdatensatzes\n",
    "Für das Training werden zum einen One-Hot-Encodings für Dezimal- und Hexadezimalwerte, und zum anderen Binärdarstellungen für die Dezimalwerte als Input verwendet.\n",
    "Die Ausgabe ist jeweils die 7-Segment Schaltung für den jeweiligen Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars9 = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "chars16 = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"A\", \"b\", \"C\", \"d\", \"E\", \"F\"]\n",
    "seven_segments9 = [seven_segment_factory_method(x) for x in chars9]\n",
    "seven_segments16 = [seven_segment_factory_method(x) for x in chars16]\n",
    "x9 = np.expand_dims(np.identity(len(chars9)), -1)\n",
    "x16 = np.expand_dims(np.identity(len(chars16)), -1)\n",
    "xb = np.expand_dims(((np.array([0,1,2,3,4,5,6,7,8,9]).reshape(-1,1) & (1 << np.arange(4))) > 0).astype(\"uint8\"), -1)\n",
    "y9 = np.expand_dims(np.array([x.getEncoding() for x in seven_segments9]), -1)\n",
    "y16 = np.expand_dims(np.array([x.getEncoding() for x in seven_segments16]), -1)\n",
    "yb = y9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erzeugen des Netzwerkes und anstoßen des Trainings\n",
    "Es werden verschiedene Netzwerkarchitekturen getestet:\n",
    "\n",
    "- Keine versteckte Ebene:\t(n $\\rightarrow$ 7)\n",
    "- Eine versteckte Ebene: \t(n $\\rightarrow$ n $\\rightarrow$ 7)\n",
    "- Eine versteckte Ebene: \t(n $\\rightarrow$ 7 $\\rightarrow$ 7)\n",
    "- Eine versteckte Ebene: \t(n $\\rightarrow$ 5 $\\rightarrow$ 7)\n",
    "- Zwei versteckte Ebenen: \t(n $\\rightarrow$ n $\\rightarrow$ n $\\rightarrow$ 7)\n",
    "- Zwei versteckte Ebenen: \t(n $\\rightarrow$ 7 $\\rightarrow$ 7 $\\rightarrow$ 7)\n",
    "- Zwei versteckte Ebenen: \t(n $\\rightarrow$ 5 $\\rightarrow$ 5 $\\rightarrow$ 7)\n",
    "- Drei versteckte Ebenen: \t(n $\\rightarrow$ n $\\rightarrow$ n $\\rightarrow$ n $\\rightarrow$ 7)\n",
    "- Drei versteckte Ebenen: \t(n $\\rightarrow$ 7 $\\rightarrow$ 7 $\\rightarrow$ 7 $\\rightarrow$ 7)\n",
    "- Drei versteckte Ebenen: \t(n $\\rightarrow$ 5 $\\rightarrow$ 5 $\\rightarrow$ 5 $\\rightarrow$ 7)\n",
    "\n",
    "Es wird für maximal 2000 Epochen trainiert. Es wurde ein verfrühtes Stoppen eingeführt, wenn sich der Fehler über einen Zeitraum von 5 Epochen nicht um mindestens 0,00001 verbessert hat. Dann wird eine Konvergenz angenommen.\n",
    "\n",
    "Die Lernrate ist auf 0,5 festgesetzt.\n",
    "\n",
    "Es wird Stochastic Gradient Descent (SGD) zum Training verwendet, also eine Batchgröße von 1. \n",
    "\n",
    "Als Fehlerfunktion wird der MSE verwendet. \n",
    "\n",
    "Als Klassifizierungsmetrik wird Accuracy verwendet. Dabei wird die Korrektheit eines Segment als TP angesehen, nicht die komplette Schaltung. Zum Vergleich wird die Ausgabe des Netzes auf 0 oder 1 gerundet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def plot_predictions(encodings:np.ndarray, image_file_name:str = None) -> None:\n",
    "\t_, axs = plt.subplots(4,4)\n",
    "\taxs = axs.flatten()\n",
    "\tfor i, encoding in enumerate(encodings):\n",
    "\t\timg = SevenSegment(encoding).getImage()\n",
    "\t\taxs[i].imshow(img)\n",
    "\tif image_file_name is not None:\n",
    "\t\tplt.savefig(image_file_name)\n",
    "\telse:\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "def test_networks(x:np.ndarray, y:np.ndarray, loss: Loss, image_file_path:str = \"\", plot_output:bool = True) -> None:\n",
    "\tnp.random.seed(0)\n",
    "\n",
    "\tnetworks: dict[list[Layer]] = {\n",
    "\t\t\"0 hidden\": [Linear(x.shape[1], 7), Sigmoid()],\n",
    "\t\tf\"1 hidden mit {x.shape[1]} Neuronen\": [Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], 7), Sigmoid()],\n",
    "\t\t\"1 hidden mit 7 Neuronen\": [Linear(x.shape[1], 7), Sigmoid(), Linear(7, 7), Sigmoid()],\n",
    "\t\t\"1 hidden mit 5 Neuronen\": [Linear(x.shape[1], 5), Sigmoid(), Linear(5, 7), Sigmoid()],\n",
    "\t\tf\"2 hidden mit {x.shape[1]}-{x.shape[1]} Neuronen\": [Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], 7), Sigmoid()],\n",
    "\t\t\"2 hidden mit 7-7 Neuronen\": [Linear(x.shape[1], 7), Sigmoid(), Linear(7, 7), Sigmoid(), Linear(7, 7), Sigmoid()],\n",
    "\t\t\"2 hidden mit 5-5 Neuronen\": [Linear(x.shape[1], 5), Sigmoid(), Linear(5, 5), Sigmoid(), Linear(5, 7), Sigmoid()],\n",
    "\t\tf\"3 hidden mit {x.shape[1]}-{x.shape[1]}-{x.shape[1]} Neuronen\": [Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], x.shape[1]), Sigmoid(), Linear(x.shape[1], 7), Sigmoid()],\n",
    "\t\t\"3 hidden mit 7-7-7 Neuronen\": [Linear(x.shape[1], 7), Sigmoid(), Linear(7, 7), Sigmoid(), Linear(7, 7), Sigmoid(), Linear(7, 7), Sigmoid()],\n",
    "\t\t\"3 hidden mit 5-5-5 Neuronen\": [Linear(x.shape[1], 5), Sigmoid(), Linear(5, 5), Sigmoid(), Linear(5, 5), Sigmoid(), Linear(5, 7), Sigmoid()],\n",
    "\t}\n",
    "\t\n",
    "\tstats:dict = dict()\n",
    "\tfor description, network in networks.items():\n",
    "\t\tmlp = MultiLayerPerceptron(network, loss)\n",
    "\t\tepoch, criteria = mlp.train(x, y, epochs=2000, lr=0.5, min_delta=1e-5, patience=5, loss_threshold=0)\n",
    "\t\tpred = mlp.predict(x)\n",
    "\t\tif plot_output:\n",
    "\t\t\tencodings = np.squeeze(np.round(pred, 0))\n",
    "\t\t\tplot_predictions(encodings, os.path.join(image_file_path, description + \".jpg\"))\n",
    "\n",
    "\t\tmetric = Accuracy()\n",
    "\t\tloss_value = loss.loss(pred, y)\n",
    "\t\tacc = metric.metric(np.round(pred,0), y)\n",
    "\t\tstats[\"Konfiguration\"] = stats.get(\"Konfiguration\", []) + [description]\n",
    "\t\tstats[\"Trainingsdauer\"] = stats.get(\"Trainingsdauer\", []) + [epoch]\n",
    "\t\tstats[\"Terminierungsgrund\"] = stats.get(\"Terminierungsgrund\", []) + [criteria]\n",
    "\t\tstats[\"Fehler\"] = stats.get(\"Fehler\", []) + [loss_value]\n",
    "\t\tstats[\"Genauigkeit\"] = stats.get(\"Genauigkeit\", []) + [acc]\n",
    "\n",
    "\treturn stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Konfiguration</th>\n",
       "      <th>Trainingsdauer</th>\n",
       "      <th>Terminierungsgrund</th>\n",
       "      <th>Fehler</th>\n",
       "      <th>Genauigkeit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 hidden</td>\n",
       "      <td>694</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 hidden mit 10 Neuronen</td>\n",
       "      <td>760</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 hidden mit 7 Neuronen</td>\n",
       "      <td>827</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 hidden mit 5 Neuronen</td>\n",
       "      <td>1058</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 hidden mit 10-10 Neuronen</td>\n",
       "      <td>1910</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 hidden mit 7-7 Neuronen</td>\n",
       "      <td>2000</td>\n",
       "      <td>Abbruch</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2 hidden mit 5-5 Neuronen</td>\n",
       "      <td>2000</td>\n",
       "      <td>Abbruch</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 hidden mit 10-10-10 Neuronen</td>\n",
       "      <td>2000</td>\n",
       "      <td>Abbruch</td>\n",
       "      <td>0.213017</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 hidden mit 7-7-7 Neuronen</td>\n",
       "      <td>897</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 hidden mit 5-5-5 Neuronen</td>\n",
       "      <td>406</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212859</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Konfiguration  Trainingsdauer Terminierungsgrund  \\\n",
       "0                        0 hidden             694         Konvergenz   \n",
       "1        1 hidden mit 10 Neuronen             760         Konvergenz   \n",
       "2         1 hidden mit 7 Neuronen             827         Konvergenz   \n",
       "3         1 hidden mit 5 Neuronen            1058         Konvergenz   \n",
       "4     2 hidden mit 10-10 Neuronen            1910         Konvergenz   \n",
       "5       2 hidden mit 7-7 Neuronen            2000            Abbruch   \n",
       "6       2 hidden mit 5-5 Neuronen            2000            Abbruch   \n",
       "7  3 hidden mit 10-10-10 Neuronen            2000            Abbruch   \n",
       "8     3 hidden mit 7-7-7 Neuronen             897         Konvergenz   \n",
       "9     3 hidden mit 5-5-5 Neuronen             406         Konvergenz   \n",
       "\n",
       "     Fehler  Genauigkeit  \n",
       "0  0.005920     1.000000  \n",
       "1  0.004446     1.000000  \n",
       "2  0.004890     1.000000  \n",
       "3  0.006484     1.000000  \n",
       "4  0.005076     1.000000  \n",
       "5  0.009050     1.000000  \n",
       "6  0.022271     0.985714  \n",
       "7  0.213017     0.671429  \n",
       "8  0.212872     0.671429  \n",
       "9  0.212859     0.671429  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = test_networks(x9, y9, MeanSquereError(), \"Images/Outputs/out9/\", plot_output=False)\n",
    "pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Konfiguration</th>\n",
       "      <th>Trainingsdauer</th>\n",
       "      <th>Terminierungsgrund</th>\n",
       "      <th>Fehler</th>\n",
       "      <th>Genauigkeit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 hidden</td>\n",
       "      <td>696</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 hidden mit 16 Neuronen</td>\n",
       "      <td>646</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 hidden mit 7 Neuronen</td>\n",
       "      <td>760</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 hidden mit 5 Neuronen</td>\n",
       "      <td>1061</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 hidden mit 16-16 Neuronen</td>\n",
       "      <td>7</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.321285</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 hidden mit 7-7 Neuronen</td>\n",
       "      <td>1498</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.049492</td>\n",
       "      <td>0.955357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2 hidden mit 5-5 Neuronen</td>\n",
       "      <td>1719</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.041447</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 hidden mit 16-16-16 Neuronen</td>\n",
       "      <td>7</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.321134</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 hidden mit 7-7-7 Neuronen</td>\n",
       "      <td>932</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.214285</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 hidden mit 5-5-5 Neuronen</td>\n",
       "      <td>394</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.214292</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Konfiguration  Trainingsdauer Terminierungsgrund  \\\n",
       "0                        0 hidden             696         Konvergenz   \n",
       "1        1 hidden mit 16 Neuronen             646         Konvergenz   \n",
       "2         1 hidden mit 7 Neuronen             760         Konvergenz   \n",
       "3         1 hidden mit 5 Neuronen            1061         Konvergenz   \n",
       "4     2 hidden mit 16-16 Neuronen               7         Konvergenz   \n",
       "5       2 hidden mit 7-7 Neuronen            1498         Konvergenz   \n",
       "6       2 hidden mit 5-5 Neuronen            1719         Konvergenz   \n",
       "7  3 hidden mit 16-16-16 Neuronen               7         Konvergenz   \n",
       "8     3 hidden mit 7-7-7 Neuronen             932         Konvergenz   \n",
       "9     3 hidden mit 5-5-5 Neuronen             394         Konvergenz   \n",
       "\n",
       "     Fehler  Genauigkeit  \n",
       "0  0.005925     1.000000  \n",
       "1  0.003173     1.000000  \n",
       "2  0.004252     1.000000  \n",
       "3  0.006491     1.000000  \n",
       "4  0.321285     0.678571  \n",
       "5  0.049492     0.955357  \n",
       "6  0.041447     0.964286  \n",
       "7  0.321134     0.678571  \n",
       "8  0.214285     0.678571  \n",
       "9  0.214292     0.678571  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = test_networks(x16, y16, MeanSquereError(), \"Images/Outputs/out16/\", plot_output=False)\n",
    "pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Konfiguration</th>\n",
       "      <th>Trainingsdauer</th>\n",
       "      <th>Terminierungsgrund</th>\n",
       "      <th>Fehler</th>\n",
       "      <th>Genauigkeit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 hidden</td>\n",
       "      <td>954</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.093637</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 hidden mit 4 Neuronen</td>\n",
       "      <td>1959</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 hidden mit 7 Neuronen</td>\n",
       "      <td>1951</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.008243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 hidden mit 5 Neuronen</td>\n",
       "      <td>1831</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.985714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 hidden mit 4-4 Neuronen</td>\n",
       "      <td>166</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212210</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 hidden mit 7-7 Neuronen</td>\n",
       "      <td>2000</td>\n",
       "      <td>Abbruch</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2 hidden mit 5-5 Neuronen</td>\n",
       "      <td>147</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212630</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 hidden mit 4-4-4 Neuronen</td>\n",
       "      <td>40</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212906</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 hidden mit 7-7-7 Neuronen</td>\n",
       "      <td>987</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212758</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 hidden mit 5-5-5 Neuronen</td>\n",
       "      <td>266</td>\n",
       "      <td>Konvergenz</td>\n",
       "      <td>0.212906</td>\n",
       "      <td>0.671429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Konfiguration  Trainingsdauer Terminierungsgrund    Fehler  \\\n",
       "0                     0 hidden             954         Konvergenz  0.093637   \n",
       "1      1 hidden mit 4 Neuronen            1959         Konvergenz  0.030939   \n",
       "2      1 hidden mit 7 Neuronen            1951         Konvergenz  0.008243   \n",
       "3      1 hidden mit 5 Neuronen            1831         Konvergenz  0.024061   \n",
       "4    2 hidden mit 4-4 Neuronen             166         Konvergenz  0.212210   \n",
       "5    2 hidden mit 7-7 Neuronen            2000            Abbruch  0.012101   \n",
       "6    2 hidden mit 5-5 Neuronen             147         Konvergenz  0.212630   \n",
       "7  3 hidden mit 4-4-4 Neuronen              40         Konvergenz  0.212906   \n",
       "8  3 hidden mit 7-7-7 Neuronen             987         Konvergenz  0.212758   \n",
       "9  3 hidden mit 5-5-5 Neuronen             266         Konvergenz  0.212906   \n",
       "\n",
       "   Genauigkeit  \n",
       "0     0.900000  \n",
       "1     0.971429  \n",
       "2     1.000000  \n",
       "3     0.985714  \n",
       "4     0.671429  \n",
       "5     1.000000  \n",
       "6     0.671429  \n",
       "7     0.671429  \n",
       "8     0.685714  \n",
       "9     0.671429  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = test_networks(xb, yb, MeanSquereError(), \"Images/Outputs/outb/\", plot_output=False)\n",
    "pd.DataFrame(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**One-Hot-Encodings** haben eine 1:1 Relation mit der Schaltung einer 7-Segmentanzeige, da jede Eingabe direkt auf ihre Ausgabe gemappt werden kann. Zur Erkennung wird hier also keine versteckte Schicht benötigt und kann mit einer linearen Schicht realisiert werden. Dabei macht es keinen Unterschied, ob die Eingabe aus 9 oder 16 oder $n$ Eingaben besteht. Auch die Geschwindigkeit bis zur Konvergenz unterscheidiet sich nicht signifikant, da nur für einen Schicht die Gradienten berechnet werden müssen und diese direkt vom Input abhängen. Daher ist die Anzahl benötigter Epochen, bis zur Konvergenz, konstant gegen die Eingabegröße.\n",
    "Gemessen an der Anzahl benötigter Parameter kann eine Eine lineare Schicht als optimal angenommen werden.\n",
    "Ein Bottleneck von 5 Neuronen in der versteckten Schicht führt auch noch zu 100% Genauigkeit, benötigt aber fast doppelt so lange zum konvergieren wie die linearen Netzwerke.\n",
    "2 versteckte Schichten konnten bei $n=9$ noch trainiert werden, 3 führen zu einer komplexeren Fehlertopographie, weshalb sich das Netzwerk schnell in lokalen Minima festfährt und konvergiert, ohne die optimale Lösung zu finden. \n",
    "\n",
    "**Binär-Kodierungen** haben keine 1:1 Relation mit der Segmentschaltung, da für die Segmente \"entweder - oder\" entschiedungen getroffen werden müssen. Daher ist ein Netzwerk mit 0 versteckten Schichten nicht in der Lage eine optimale Lösung zu finden. Die beste Konfiguration, gemessen an der Anzahl Parameter, ist eine versteckte Schicht mit 7 Neuronen.\n",
    "Auch hier zeigt sich, dass bei 3 versteckte Schichten, sich das Netzwerk in lokalen Minima verfährt und konvergiert, ohne ein globales Optimum zu finden. Insgesammt lässt sich vermuten, dass das Netzwerk vorm letzten Schritt eine versteckte Schicht haben muss mit mindestens der selben Neuronenzahl, wie Ausgänge, um die $xor$ Verknüpfungen zu realisieren. Die Begründung ist, dass nur die beiden Netzwerke mit 7 Neuronen in der letzten (versteckten) Schicht in der Lage waren eine Accuracy von 100% zu erreichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2\n",
    "Frage: Was eignet sich zur Klassifikation besser, der mittlere quadratische Fehler (MSE) oder die categorival-cross-entropy (CCE)?\n",
    "\n",
    "Antwort: Die CCE benötigt einen Wahrscheinlichkeitsvektor als Eingabe, um zu funktionieren. \n",
    "Der Grund liegt darin, dass $y_i \\cdot \\log(\\hat{y})$ 0 ergibt für alle Ergebnisse, die eine 0 als erwartete Ausgabe haben. \n",
    "Wenn $\\hat{y}$ ein Wahrscheinlichkeitsvektor ist, hängt $\\hat{y}_i$ von den anderen Stellen im Vektor ab. \n",
    "Haben die anderen Stellen einen hohen Wert, so muss der Wert von $\\hat{y}_i$ zwangsläufig klein sein und führt somit zu einem hohen Fehler.\n",
    "Die CCE eignet sich also nur für Probleme, bei denen genau eine Klasse vorhergesagt werden soll.\n",
    "Das vorliegende Problem ist aber ein Mehrklassenproblem, in dem jedes Ausgabeneuron 0 oder 1 schalten kann, unabhängig von den anderen.\n",
    "Nutzt man CCE als Fehlerfunktion, so lernt das Netzwerk, dass es alle Gewichte auf 1 setzen muss, da so das Ergebniss immer 1 und der Fehler nahe 0 ist, weil die Stellen, die 0 sein sollten, nicht mit in den Fehler einfließen.\n",
    "\n",
    "Als Alternative kann die Binary-Cross-Entropy (BCE) verwendet werden:\n",
    "$$\n",
    "\\frac{1}{|X|}\\sum_{x\\in X} -(y \\cdot \\log(x) + (1-y) \\cdot \\log(1-x))\n",
    "$$\n",
    "BCE ist quasi die CCE pro Ausgabe, bestehend aus Wahrscheinlichkeit und Gegenwahrscheinlichkeit.\n",
    "\n",
    "Als Antwort auf dei Frage lässt sich somit per Ausschluss sagen, dass MSE besser geeignet ist für dieses Problem, als CCE, da die Vorraussetzung für CCE nicht gegeben ist!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('as')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6050835552411db202700dd31d1157379fd9d2f59e0bf949368933b350f4c51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
